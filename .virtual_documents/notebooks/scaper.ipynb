# Standard library
import re
import sys
import json
import requests
from time        import time, sleep
from pathlib     import Path
from importlib   import reload
from collections import Counter
sys.path.append(str((Path(".").resolve().parent / "src").resolve()))

# Extenal libraries + own
from bs4 import BeautifulSoup, element
import scraper_utils as utils
utils = reload(utils)

# Selenium
import selenium
from   selenium import webdriver
from   selenium.webdriver.common.action_chains import ActionChains
from   selenium.webdriver.common.keys import Keys
from   selenium.webdriver.common.by   import By
from   selenium.webdriver.support.ui  import Select
from   selenium.common.exceptions     import ElementClickInterceptedException


fruit_url = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/fruitandveg-essentials"
resp = requests.get(fruit_url, headers={"content-type":"application/json"})
print(resp.status_code)


with open("fruit_essentials.html", "w") as f:
    f.write(resp._content.decode("utf-8"))


soup = BeautifulSoup(resp._content.decode("utf-8"), 'html.parser')


with open("fruit_essentials_bs4.html", "w") as f:
    f.write(soup.prettify())
vars(soup.body).keys()


page = utils.fetch_tag(soup.body, "page") # soup.body.contents[1]
page2 = soup.body.div
page3 = soup.find_all(id="page")[0]


hash(page) == hash(page2), hash(page2) == hash(page3)


page2.attrs == {'id':'page'}


grid_items_ = page.find_all(attrs={"class":"gridItem"})
len(grid_items_)


g = grid_items_[0]
type(g)


vars(g).keys()


g.div.div.div.h3


list(enumerate(g.div.div.div.h3.descendants))


[
    (i,item) for i,item in 
    enumerate(grid_items_[5].find_all("h3")[0].descendants)
    if item != "\n"
]


"\r\n                                                Sainsbury's Red Pepper\r\n                                                ".strip()


Counter(
    len(list(g.find_all("h3")[0].descendants))
    for g in grid_items_
)


get_ipython().getoutput("wget "https://assets.sainsburys-groceries.co.uk/gol/1196757/image.jpg" -o banana.jpg")


req = requests.get("https://assets.sainsburys-groceries.co.uk/gol/1196757/image.jpg")
req.status_code


Path("banana.jpg").write_bytes(req.content)


## Way too hard-coded

# g.div.div.next_sibling.next_sibling.next_sibling.next_sibling.div.div.div.div
# list(enumerate(g.div.div.next_sibling.next_sibling.next_sibling.next_sibling.div.div.div.div.descendants))


g.find_all(attrs={"class":"pricePerUnit"})[0].string.strip()


''.join(g.find_all(attrs={"class":"pricePerUnit"})[0].strings).strip()


g.find_all(attrs={"class":"pricePerMeasure"})


list(enumerate(
    g
    .find_all(attrs={"class":"pricePerMeasure"})[0]
    .descendants
))


tmp = [
    (list((
    g
    .find_all(attrs={"class":"pricePerMeasure"})[0]
    .descendants
    )))
    for g in grid_items_
]


[t for t in tmp if len(t) == 8]


[
    ''.join(
        grid_item
        .find_all(attrs={"class":"pricePerMeasure"})[0].strings
    ).strip()
    for grid_item in grid_items_
][:5]


[
    ''.join(
        grid_item
        .find_all(attrs={"class":"pricePerMeasure"})[0].strings
    ).strip()
    for grid_item in grid_items_
][:5]


list(enumerate(
    g
    .find_all(attrs={"class":"pricePerMeasure"})[0]
    .find_all(attrs={"class":"pricePerMeasureMeasure"})#[0]
))


import sys
import requests
from collections import Counter
from time import time
from pathlib import Path
from importlib import reload

import pandas as pd
from bs4 import BeautifulSoup, element
from matplotlib_venn import venn2, venn3
import matplotlib.pyplot as plt

sys.path.append(str((Path(".").resolve().parent / "src").resolve()))


import scraper_utils as utils
utils = reload(utils)


fruit_url = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/fruitandveg-essentials"
t0 = time()
resp = requests.get(fruit_url)#, headers={"content-type":"application/json"})
print(f"Page downloaded in {time() - t0:.6f}s")
soup = BeautifulSoup(resp._content.decode("utf-8"), 'html.parser')


items = utils.scrape_items(soup)
len(items)


items.head()


fruit_url2 = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/CategoryDisplay?langId=44&storeId=10151&catalogId=10241&categoryId=474593&orderBy=TOP_SELLERS%7CSEQUENCING&beginIndex=0&promotionId=&listId=&searchTerm=&hasPreviousOrder=&previousOrderId=&categoryFacetId1=&categoryFacetId2=&ImportedProductsCount=&ImportedStoreName=&ImportedSupermarket=&bundleId=&parent_category_rn=12518&top_category=12518&pageSize=120#langId=44&storeId=10151&catalogId=10241&categoryId=474593&parent_category_rn=12518&top_category=12518&pageSize=120&orderBy=TOP_SELLERS%7CSEQUENCING&searchTerm=&beginIndex=0&facet="
t0 = time()
resp = requests.get(fruit_url2)#, headers={"content-type":"application/json"})
print(f"Page downloaded in {time() - t0:.6f}s")
soup = BeautifulSoup(resp._content.decode("utf-8"), 'html.parser')
items = utils.scrape_items(soup)
items.shape



fruit_url3 = "https://www.sainsburys.co.uk/shop/CategoryDisplay?listId=&catalogId=10241&searchTerm=&beginIndex=120&pageSize=120&orderBy=TOP_SELLERS%7CSEQUENCING&top_category=12518&langId=44&storeId=10151&categoryId=474593&promotionId=&parent_category_rn=12518"
t0 = time()
resp = requests.get(fruit_url3)#, headers={"content-type":"application/json"})
print(f"Page downloaded in {time() - t0:.6f}s")
soup = BeautifulSoup(resp._content.decode("utf-8"), 'html.parser')
items = utils.scrape_items(soup)
items.shape


import pandas as pd


df_pq = pd.read_parquet("../data/sainsburys-prices.parquet")
df_pq.shape


df_pq.head()


base_link = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/british-produce"
browser_link = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/CategoryDisplay?langId=44&storeId=10151&catalogId=10241&categoryId=484866&orderBy=SEQUENCING%7CFAVOURITES_ONLY%7CTOP_SELLERS&beginIndex=0&promotionId=&listId=&searchTerm=&hasPreviousOrder=&previousOrderId=&categoryFacetId1=&categoryFacetId2=&ImportedProductsCount=&ImportedStoreName=&ImportedSupermarket=&bundleId=&parent_category_rn=12518&top_category=12518&pageSize=120#langId=44&storeId=10151&catalogId=10241&categoryId=484866&parent_category_rn=12518&top_category=12518&pageSize=120&orderBy=SEQUENCING%7CFAVOURITES_ONLY%7CTOP_SELLERS&searchTerm=&beginIndex=0&hideFilters=true"
base_hacked_link = base_link + "?langId=44&pageSize=120"



req1 = requests.get(base_link)
req2 = requests.get(browser_link)
req3 = requests.get(base_hacked_link)
# len(req1),len(req2),len(req3)


items1 = utils.scrape_items(BeautifulSoup(req1._content.decode("utf-8"), 'html.parser'))
items2 = utils.scrape_items(BeautifulSoup(req2._content.decode("utf-8"), 'html.parser'))
items3 = utils.scrape_items(BeautifulSoup(req3._content.decode("utf-8"), 'html.parser'))




items1.shape, items2.shape, items3.shape, 


base_link = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/fresh-fruit"
browser_link = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/all-fruit#langId=44&storeId=10151&catalogId=10241&categoryId=12545&parent_category_rn=12518&top_category=12518&pageSize=60&orderBy=FAVOURITES_ONLY%7CSEQUENCING%7CTOP_SELLERS&searchTerm=&beginIndex=0&hideFilters=true"
base_hacked_link = base_link + "?langId=44&pageSize=120"



req1 = requests.get(base_link)
req2 = requests.get(browser_link)
req3 = requests.get(base_hacked_link)
# len(req1),len(req2),len(req3)


soup = BeautifulSoup(req1.content.decode(), 'html.parser')



Path("fresh-fruit-home.html").write_text(soup.prettify())


# list(enumerate(soup.children))


items1 = utils.scrape_items(BeautifulSoup(req1._content.decode("utf-8"), 'html.parser'))
items2 = utils.scrape_items(BeautifulSoup(req2._content.decode("utf-8"), 'html.parser'))
items3 = utils.scrape_items(BeautifulSoup(req3._content.decode("utf-8"), 'html.parser'))




items1.shape, items2.shape, items3.shape, 


base_link = "https://www.sainsburys.co.uk/shop/gb/groceries"
req1 = requests.get(base_link)
soup = BeautifulSoup(req1.content.decode(), 'html.parser')
Path("groceries-home.html").write_text(soup.prettify())


len(list(soup.find_all(attrs = {'class':'megaNavListItem'})))


list(enumerate(soup.find_all(attrs = {'class':'megaNavListItem'})))[10:13]


dict(
    (str(i),i) if i%2==0 else (None,None) for i in range(5)
)


mega_nav = list((soup.find_all(attrs = {'class':'megaNavListItem'})))[11]
print(type(mega_nav), "\n")
list(enumerate(mega_nav.descendants))


list(mega_nav.descendants)[1].attrs


for i,x in enumerate(soup.find_all(attrs = {'class':'megaNavListItem'})):
    stringos = [s.strip() for s in x.strings if s != "\n"]
    print(i, stringos)


list(list(mega_nav.descendants)[1].strings)


[(type(x),x) for x in mega_nav.children]


vars(list(mega_nav.children)[0])


getattr(list(mega_nav.children)[1], 'attrs', 'skere')#['href']


vars(list(mega_nav.children)[1])


import scraper_utils as utils
utils = reload(utils)


from time import sleep


children = utils.navigate_root(utils.sainburys_home)
children


children_pages = {}
for title,link in children.items():
    sleep(1)
    if link is None:
        continue
    if not link.startswith('https://www.sainsburys.co.uk/'):
        continue
    children_pages[title] = requests.get(link)


children_soups = {
    t:BeautifulSoup(resp.content.decode(), 'html.parser')
    for t,resp in children_pages.items()
}


fruit = children_soups['Fruit & vegetables']
Path('lvl1-fruit.html').write_text(fruit.prettify())


tmp = fruit.find_all(attrs = {'id':"breadcrumbNav"})
fruit_breadcrumb = list(tmp)[0]
list(enumerate(
    list(
        list(
            list(fruit_breadcrumb.children)[3]
            .children
        )[1]
    )#[5]#.children
))


i = 4
list(enumerate(fruit_breadcrumb.descendants))[(i*5):((i+1)*5)]
x = list(fruit_breadcrumb.descendants)[21]
x


x.contents[0].attrs


x.name


([
    item for item in
    fruit_breadcrumb.descendants
    if getattr(item, 'name', '') == 'li'
    and hasattr(getattr(item, 'contents', '')[0], 'attrs')
    and getattr(item, 'contents')[0].attrs.get('href', '')
])


layer2 = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/fruitandveg-essentials"
layer2_req = requests.get(layer2)
fruit_ess = BeautifulSoup(layer2_req.content.decode(), 'html.parser')


Path('lvl2-fruit-essentials.html').write_text(fruit_ess.prettify())


grid_items = list(fruit_ess.find_all(attrs={"class":"gridItem"}))
len(grid_items)


layer2_req_vars = vars(layer2_req)
list(layer2_req_vars.keys())


redir = vars(layer2_req_vars['history'][0])['headers']['Location']


redir_mod_1 = re.sub(r'(storeId=\d{1,7}&)',r'\1beginIndex=0&pageSize=120&',redir)
redir_mod_2 = re.sub(r'(storeId=\d{1,7}&)',r'\1beginIndex=60&pageSize=120&',redir)
redir_mod_3 = re.sub(r'(storeId=\d{1,7}&)',r'\1beginIndex=120&pageSize=120&',redir)
redir_mod_4 = re.sub(r'(storeId=\d{1,7}&)',r'\1beginIndex=180&pageSize=120&',redir)


fruit_ess_2_req_1 = requests.get(redir_mod_1)
fruit_ess_2_req_2 = requests.get(redir_mod_2)
fruit_ess_2_req_3 = requests.get(redir_mod_3)
fruit_ess_2_req_4 = requests.get(redir_mod_4)


lines_1 = set(fruit_ess_2_req_1.content.decode().split("\n"))
lines_2 = set(fruit_ess_2_req_2.content.decode().split("\n"))
len(lines_1), len(lines_2), len(lines_1&lines_2), len(lines_1|lines_2)


fruit_ess_2_bs_1 = BeautifulSoup(fruit_ess_2_req_1.content.decode(), 'html.parser')
fruit_ess_2_bs_2 = BeautifulSoup(fruit_ess_2_req_2.content.decode(), 'html.parser')
fruit_ess_2_bs_3 = BeautifulSoup(fruit_ess_2_req_3.content.decode(), 'html.parser')
fruit_ess_2_bs_4 = BeautifulSoup(fruit_ess_2_req_4.content.decode(), 'html.parser')


print(
    len(list(fruit_ess_2_bs_1.find_all(attrs={"class":"gridItem"}))),
    len(list(fruit_ess_2_bs_2.find_all(attrs={"class":"gridItem"}))),
    len(list(fruit_ess_2_bs_3.find_all(attrs={"class":"gridItem"}))),
    len(list(fruit_ess_2_bs_4.find_all(attrs={"class":"gridItem"})))
)


def scrape_item_list(grid_items):
    items = []
    for i,g in enumerate(grid_items):
        title, thubmnail = utils.scape_item_thumbnail(g)
        price_unit       = utils.scrape_price_per_unit(g)
        price_measure    = utils.scrape_price_per_measure(g)
        items.append({
            'title'         : title,
            'thubmnail'     : thubmnail,
            'price_unit'    : price_unit,
            'price_measure' : price_measure
        })
    items_df = (
        pd.DataFrame.from_dict(items)
        .assign(scraping_date = str(datetime.now()))
    )
    return items_df


fruit_ess_2_bs_1_df = scrape_item_list(fruit_ess_2_bs_1.find_all(attrs={"class":"gridItem"}))
fruit_ess_2_bs_2_df = scrape_item_list(fruit_ess_2_bs_2.find_all(attrs={"class":"gridItem"}))
fruit_ess_2_bs_3_df = scrape_item_list(fruit_ess_2_bs_3.find_all(attrs={"class":"gridItem"}))
fruit_ess_2_bs_4_df = scrape_item_list(fruit_ess_2_bs_4.find_all(attrs={"class":"gridItem"}))


plt.figure(figsize = (16,8), facecolor="white")
plt.subplot(121)
venn3([
    set(fruit_ess_2_bs_1_df['title']),
    set(fruit_ess_2_bs_2_df['title']),
    set(fruit_ess_2_bs_3_df['title'])
], (1,2,3))
plt.subplot(122)
venn3([
    set(fruit_ess_2_bs_4_df['title']),
    set(fruit_ess_2_bs_2_df['title']),
    set(fruit_ess_2_bs_3_df['title'])
], (4,2,3))


fruit_ess_2_bs_1.find_all(attrs = {'id':"resultsHeading"})


# layer2 = "https://www.sainsburys.co.uk/shop/gb/groceries/fruit-veg/fruitandveg-essentials"
layer0 = "https://www.sainsburys.co.uk/shop/gb/groceries"
driver = webdriver.Firefox()
# driver.get(layer0)


a = ActionChains(driver)
driver.get(layer0)
driver.title
sleep(5)
# Turn into Explicit Wait
try:
    cookies_button_xpath = """//*[@id="onetrust-accept-btn-handler"]"""
    accept_cookies_button = driver.find_element(By.XPATH, cookies_button_xpath)
    accept_cookies_button.click()
except:
    pass


elem = driver.find_element(By.ID, "pageSize")
elem


xpath = '//*[@id="pageSize"]'
elem = driver.find_element(By.XPATH, xpath)
elem


[
    e.get_attribute('value') for e in
    elem.find_elements(By.TAG_NAME, 'option')
]


select = Select(elem)
try:
    select.select_by_value('120')
except ElementClickInterceptedException as e:
    cookies_button_xpath = """//*[@id="onetrust-accept-btn-handler"]"""
    accept_cookies_button = driver.find_element(By.XPATH, cookies_button_xpath)
    accept_cookies_button.click()
finally:
    select.select_by_value('120')



elem = driver.find_element(By.ID, "pageSize")


next_page_xpath = """/html/body/div[8]/div[2]/div[1]/div[4]/div[2]/div[1]/ul[2]/li[3]/a"""
next_page = driver.find_element(By.XPATH, next_page_xpath)
next_page


next_page_xpath = """/html/body/div[8]/div[2]/div[1]/div[4]/div[2]/div[1]/ul[2]/li[3]/a"""
next_page = driver.find_element(By.XPATH, next_page_xpath)
next_page


next_page.click()


# elem.clear()
# elem.send_keys("pycon")
# elem.send_keys(Keys.RETURN)
# assert "No results found." not in driver.page_source
# driver.close()


driver.close()


main_arrow_x = """//*[@id="mainNavArrow"]"""
arrow = driver.find_element(By.XPATH, main_arrow_x)
arrow


# main_arrow_x = """//*[@id="mainNavArrow"]"""
arrow = driver.find_element(By.LINK_TEXT, "Groceries")
arrow


a = ActionChains(driver)
#hover over element
a.move_to_element(arrow).perform()


fruit_in_menu = driver.find_element(By.LINK_TEXT, "Fruit & vegetables")
fruit_in_menu


fruit_in_menu_xpath = """/html/body/div[8]/div[1]/div/div[3]/div[2]/div/div[2]/ul[2]/li[8]/a"""
fruit_in_menu = driver.find_element(By.XPATH, fruit_in_menu_xpath)
fruit_in_menu


a.move_to_element(fruit_in_menu).perform()


arrow = driver.find_element(By.LINK_TEXT, "Groceries")
a.move_to_element(arrow).perform()


# groceries_list = driver.find_elements(By.CLASS_NAME, "megaNavLink.megaNavActiveLink")
groceries_list = [
    item for item in
    driver.find_elements(By.CLASS_NAME, "megaNavListItem")
    if item.rect['x'] != 0 and item.rect['x'] != 0
]
# groceries_list[0].click()
# groceries_list.is_displayed()
len(groceries_list)


[(i, item.text, item.rect) for i,item in enumerate(groceries_list)]


a.move_to_element(groceries_list[1]).perform()


a.move_to_element(groceries_list[5]).perform()


a.move_to_element(groceries_list[5]).click().perform()


groceries_list_lvl1 = [
            item for item in
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['x'] != 0
        ]


[f"{item.text:>30}|{item.rect})" for item in groceries_list_lvl1]


gro_dict = {item.text:item for item in 
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['x'] != 0}


# key = "Fresh salad"
key = "Fruit & veg essentials"


gro_dict[key].text


gro_dict[key].get_attribute('href')


gro_dict[key].get_attribute('value')


groceries_list_lvl1[20].text


a.move_to_element(groceries_list_lvl1[20]).perform()
groceries_list_lvl1[20].click()


for item in groceries_list_lvl1:
    if item.text == "Fresh fruit":
        a.move_to_element(item).perform()


groceries_list_lvl2 = [
        item for item in
        driver.find_elements(By.CLASS_NAME, "megaNavListItem")
        if item.rect['x'] != 0 and item.rect['x'] != 0
    ]
[f"{item.text:>30}|{item.rect})" for item in groceries_list_lvl2]


driver.execute_script("window.scrollTo(0,0)")
arrow = driver.find_element(By.LINK_TEXT, "Groceries")
a.move_to_element(arrow).perform()
sleep(0.25)
print("number of megaNavListItem:", len(megaNavListItem))

groceries_list = [
    item for item in
    driver.find_elements(By.CLASS_NAME, "megaNavListItem")
    if item.rect['x'] != 0 and item.rect['y'] != 0
]
print("number of valid items:", len(groceries_list))

height_offset = 50
for i,gro_item in enumerate(groceries_list):
    sleep(0.75)
    try:
        a.move_to_element(gro_item).perform()
        groceries_list_lvl1 = [
            item for item in
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['y'] != 0
        ]
        print(f"[{i:2}]{gro_item.text:>25} | {len(groceries_list_lvl1)} categories visible")
    except:
        sleep(0.5)
        # driver.execute_script("arguments[0].scrollIntoView(true);", gro_item)
        driver.execute_script(f"window.scrollTo(0,{height_offset})")
        height_offset += 50
        sleep(0.5)
        groceries_list_lvl1 = [
            item for item in
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['y'] != 0
        ]
        
        try:
            cat_name = gro_item.text
        except:
            cat_name = 'error'
        print(f"[{i:2}]{cat_name:>25} | {len(groceries_list_lvl1)} categories visible")
    


driver.execute_script("window.scrollTo(0,0)")
arrow = driver.find_element(By.LINK_TEXT, "Groceries")
a.move_to_element(arrow).perform()
sleep(0.25)

groceries_list = [
    item for item in
    driver.find_elements(By.CLASS_NAME, "megaNavListItem")
    if item.rect['x'] != 0 and item.rect['y'] != 0
]
print("number of megaNavListItem found:", len(groceries_list))

groceries_list_valid = []
category_names = []
for gro_item in groceries_list:
    try:
        category_names.append(gro_item.text)
        groceries_list_valid.append(gro_item)
    except:
        pass
print("number of valid megaNavListItem:", len(groceries_list_valid))

current_categories = len(groceries_list_valid)
height_offset = 50
for i,gro_item in enumerate(groceries_list_valid):
    sleep(0.75)
    try:
        a.move_to_element(gro_item).perform()
        groceries_list_sub = [
            item for item in
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['y'] != 0
        ]
    except:
        sleep(0.5)
        # driver.execute_script("arguments[0].scrollIntoView(true);", gro_item)
        driver.execute_script(f"window.scrollTo(0,{height_offset})")
        try:
            a.move_to_element(gro_item).perform()
        except:
            pass
        height_offset += 50
        sleep(0.5)
        groceries_list_sub = [
            item for item in
            driver.find_elements(By.CLASS_NAME, "megaNavListItem")
            if item.rect['x'] != 0 and item.rect['y'] != 0
        ]
    try:
        cat_name = gro_item.text
    except:
        continue
    sub_categories_names = []
    for gro_sub_item in groceries_list_sub:
        try:
            sub_categories_names.append(gro_sub_item.text)
        except:
            pass
    new_categories = set(sub_categories_names) - set(category_names)
    print(f"{i:-^80}")
    print(f"{f'New categories for {cat_name}':^80}")
    print(new_categories)
    # print(f"[{i:2}]{cat_name:>25} | {len(groceries_list_sub)} categories visible")
    


def get_valid_category_names(categories_list):
    categories_list_valid = []
    category_names = []
    for gro_item in categories_list:
        try:
            cat_name = gro_item.text # this usually triggers an error
        except:
            continue
        category_names.append(cat_name)
        categories_list_valid.append(gro_item)
    return categories_list_valid, category_names
    
def find_mega_na_list(driver):
    categories_list = [
        item for item in driver.find_elements(By.CLASS_NAME, "megaNavListItem")
        if item.rect['x'] != 0 and item.rect['y'] != 0
    ]
    return categories_list
    
def fetch_meganavlist(driver, categories_list, height_offset=0):
    categories_list_valid, category_names = get_valid_category_names(categories_list)
    n_current_cats = len(categories_list_valid)
    print("n_current_cats", n_current_cats)
    directory_tree = {}
    for i,gro_item in enumerate(categories_list_valid):
        sleep(0.75)
        try:
            a.move_to_element(gro_item).perform()
            categories_list_sub = find_mega_na_list(driver)
        except:
            sleep(0.5)
            driver.execute_script(f"window.scrollTo(0,{height_offset})")
            height_offset += 50
            sleep(0.5)
            categories_list_sub = find_mega_na_list(driver)
        
        (categories_list_sub_valid, 
                 category_names_sub) = get_valid_category_names(categories_list_sub)
        try:
            print(
                f"[{i:2}]{gro_item.text:>30} | {len(categories_list_sub)} categories visible "
                f"({len(categories_list_sub_valid)} of which are valid)"
            )
        except:
            print("skippinggg")
            continue
        
        # Case 1: new categories appear when hovering: depth first search
        if len(categories_list_sub_valid) > n_current_cats:
            new_categories = set(category_names_sub) - set(category_names)
            # new_categories_items = [
            #     categ_item for categ_name, categ_item in
            #     zip(category_names_sub, categories_list_sub_valid)
            #     if categ_name in new_categories
            # ]
            print(f"----> `{gro_item.text}` has more items than baseline, going one level down in recursion.")
            # print(f"----> There are {len(new_categories)} new category names and {len(new_categories_items)} items")
            print(f"----> There are {len(new_categories)} new categories")
            sleep(0.5)
            sub_directory_tree = fetch_meganavlist(driver, categories_list_sub_valid, height_offset)
            directory_tree.update(sub_directory_tree)
        # Case 2: No new categories appear: node encountered (base case)
        else:
            directory_tree[gro_item.text] = category_names_sub 
    return directory_tree

def navigate_categories(driver):
    driver.execute_script("window.scrollTo(0,0)")
    arrow = driver.find_element(By.LINK_TEXT, "Groceries")
    a.move_to_element(arrow).perform()
    sleep(0.25)
    categories_list = [
        item for item in
        driver.find_elements(By.CLASS_NAME, "megaNavListItem")
        if item.rect['x'] != 0 and item.rect['y'] != 0
    ]
    print("Number of megaNavListItem found:", len(categories_list))
    height_offset=0
    directory_tree = fetch_meganavlist(driver, categories_list, height_offset)
    return directory_tree



directory_tree = navigate_categories(driver)


# directory_tree


new_dir = {}
lvl0 = directory_tree.keys()
for k,v in directory_tree.items():
    non_redundant_keys = list(set(v) - set(lvl0))
    new_dir[k] = {key:[key] for key in non_redundant_keys}


Path("directory-tree-0.json").write_text(
    json.dumps(new_dir, indent=2)
)


driver.execute_script("window.scrollTo(0,0)")
arrow = driver.find_element(By.LINK_TEXT, "Groceries")
a.move_to_element(arrow).perform()
sleep(0.25)

groceries_list = [
    item for item in
    driver.find_elements(By.CLASS_NAME, "megaNavListItem")
    if item.rect['x'] != 0 and item.rect['y'] != 0
]
print("number of megaNavListItem found:", len(groceries_list))

groceries_list_valid = []
category_names = []
for gro_item in groceries_list:
    try:
        category_names.append(gro_item.text)
        groceries_list_valid.append(gro_item)
    except:
        pass
print("number of valid megaNavListItem:", len(groceries_list_valid))



driver.current_url


x = driver.page_source
print(len(x), type(x))
del x


selenium_source_soup = BeautifulSoup(driver.page_source)


selenium_items = utils.scrape_items(selenium_source_soup)
selenium_items.shape


opts = webdriver.FirefoxOptions()
opts.headless = True
firefox = webdriver.Firefox(options=opts)


firefox.get(layer2)
firefox.title


len(firefox.find_elements(By.CLASS_NAME, "gridItem"))


# arrow.click() # click is not what we want, we want hover


firefox.close()


dirtree = {
    'fruit-veggies':[
        'fresh salad',
        'fresh etc'
    ],
    'meat': [
        'vegan', 
        {
            'bbq':['beef', 'chicken', 'pork']
        }
    ]
}


def initialise_checker(dirtree):
    """Given a nested dict of mixed lists and strings,
    return a dictionary where the keys are the nodes 
    and all values are set to False."""
    status_dict = {}
    if isinstance(dirtree, dict):
        for key,value in dirtree.items():
            status_dict.update( initialise_checker(value) )
    elif isinstance(dirtree, list):
        string_values = [val for val in dirtree if isinstance(val,str)]
        status_dict.update({key:False for key in string_values})
        dict_values   = [val for val in dirtree if isinstance(val,dict)]
        for inner_dict in dict_values:
            status_dict.update( initialise_checker(inner_dict) )
    return status_dict


initialise_checker(dirtree)


utils = reload(utils)
page = utils.fetch_tag(soup.body, "page") # soup.body.contents[1]
main = utils.fetch_tag(page, "main")
content_article = utils.fetch_tag(main, "content")
products_container = utils.fetch_tag(content_article, "productsContainer")
product_lister = utils.fetch_tag(products_container, "productLister")
prod_lister_gridview = utils.fetch_tag(product_lister, ['productLister', 'gridView'], "class")
Counter([type(x) for x in prod_lister_gridview.contents])

[x.attrs for x in prod_lister_gridview.contents if isinstance(x, element.Tag)][:6]


len(page.find_all(attrs = {"class":"productNameAndPromotions"}))


grid_items = [
    x for x in prod_lister_gridview.contents 
    if isinstance(x, element.Tag) and x.attrs.get("class") == ["gridItem"]
]
print(len(grid_items))
grid_items[0].attrs



g = grid_items[0]
Counter([type(x) for x in g.contents])


g.div.div.div.a.text.strip()


g.find_all(attrs = {"class":"productNameAndPromotions"})


g.find_all(attrs = {"class":"priceTab activeContainer priceTabContainer"})


grid_items_ = prod_lister_gridview.find_all(attrs={"class":"gridItem"})
len(grid_items_)
